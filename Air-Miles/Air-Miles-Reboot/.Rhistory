filesPath <- c("Z:\\SILVERPOP\\Silverpop_Files_Created\\Databases", "Z:\\SILVERPOP\\Silverpop_Files_Created\\Relational_Tables")
reqFiles <- unlist(sapply(filesPath, function(x)list.files(path = x, pattern = ".TXT$", full.names = TRUE)))
reqFiles
unname(reqFiles)
reqFiles[!grepl(pattern = "STATEMENT|SEGMENTATION", x = reqFiles)]
unname(reqFiles[!grepl(pattern = "STATEMENT|SEGMENTATION", x = reqFiles)])
unname(reqFiles)
library(httr)
library(dplyr)
#Copying required credentials from Silverpop
##Application Name:    Ali's app for API access
##Client ID:	2fb47a91-3e34-4b2f-b9b5-93cffbf31500
##Client Secret:	5b5aa1d0-9fcd-4740-b970-79b059c32a74
##Refresh Token: rDI5XuVASIqwIOOeJdRHYC71vrLvci64kE1JgDipG8BAS1
#The following from IBM website is incorrect, use the alternate provided
ibmUrl <- "https://api3.ibmmarketingcloud/oauth/token"
ibmUrl <- "https://api3.ibmmarketingcloud.com/oauth/token"
clientId <- "2fb47a91-3e34-4b2f-b9b5-93cffbf31500"
clientSecret <- "5b5aa1d0-9fcd-4740-b970-79b059c32a74"
refreshToken <- "rDI5XuVASIqwIOOeJdRHYC71vrLvci64kE1JgDipG8BAS1"
#Request authentication (access token)
reqAccess <- POST(url = ibmUrl,
body = list(grant_type = "refresh_token", client_id = clientId,
client_secret = clientSecret, refresh_token = refreshToken), verbose())
warn_for_status(reqAccess)
access_token <- content(reqAccess)$access_token
#database/table id <- 3874161
#account_no <- 4241
<SelectRecipientData>
<LIST_ID>45654</LIST_ID>
<EMAIL>someone@adomain.com</EMAIL>
<COLUMN>
<NAME>Customer Id</NAME>
<VALUE>123-45-6789</VALUE>
</COLUMN>
</SelectRecipientData>
</Body>
</Envelope>
ibmUrl <- "api3.silverpop.com/XMLAPI"
body1 <- "<Envelope><Body>
<Login>
<USERNAME>aliarsalan.kazmi@aimia.com</USERNAME>
<PASSWORD>Iamneo456_</PASSWORD>
</Login>
</Body></Envelope>"
body1
test1 <- POST(url = ibmUrl,
body = body1, verbose())
content(test1)
test1 <- POST(url = ibmUrl,
body = body1, verbose())
test1 <- POST(url = ibmUrl,
body = body1, verbose(), add_headers(Content-Length: 225))
?add_header
?add_headers
test1 <- POST(url = ibmUrl,
body = body1, verbose(), add_headers(Content-Length = 225))
test1 <- POST(url = ibmUrl,
body = body1, verbose(), add_headers(Content-Type = "application/x-www-form-urlencoded"))
test1 <- POST(url = ibmUrl,
body = body1, verbose(), add_headers(Content-Type = "application/x-www-form-urlencoded"))
GET("http://httpbin.org/headers",
add_headers(version = version$version.string), verbose())
r1 <- POST(url = ibmUrl,
body = list(VISIBILITY = 1, LIST_TYPE = 2, grant_type = "refresh_token", client_id = clientId,
client_secret = clientSecret, refresh_token = refreshToken), verbose(), add_headers(Authorization = access_token))
r1 <- POST(url = ibmUrl,
body = list(VISIBILITY = 1, LIST_TYPE = 2, grant_type = "refresh_token", client_id = clientId,
client_secret = clientSecret, refresh_token = refreshToken), verbose(), add_headers(Authorization = access_token, Content-Type = "application/x-www-form-urlencoded"))
r1 <- POST(url = ibmUrl,
body = list(VISIBILITY = 1, LIST_TYPE = 2, grant_type = "refresh_token", client_id = clientId,
client_secret = clientSecret, refresh_token = refreshToken), verbose(), add_headers(Authorization = access_token))
r1 <- POST(url = ibmUrl,
body = list(VISIBILITY = 1, LIST_TYPE = 2, grant_type = "refresh_token", client_id = clientId,
client_secret = clientSecret, refresh_token = refreshToken), verbose(), add_headers(Content-Type = access_token))
test1 <- POST(url = ibmUrl,
body = body1, verbose(), content_type("application/x-www-form-urlencoded"))
content(test1)
test1 <- POST(url = ibmUrl,
body = body1, verbose(), content_type("application/x-www-form-urlencoded"))
?POST
test1 <- POST(url = ibmUrl,
body = body1, verbose(), content_type_xml())
content(test1)
test1 <- POST(url = ibmUrl,
body = body1, verbose(), content_type("application/x-www-form-urlencoded"))
content(test1)
library(httr)
r <- POST("http://www.datasciencetoolkit.org/text2people",
body = "Tim O'Reilly, Archbishop Huxley")
stop_for_status(r)
content(r, "parsed", "application/json")
install.packages("httpRequest")
handle <- getCurlHandle()
?getCurl
library(RCurl)
handle <- getCurlHandle()
handle
handle <- getCurlHandle()
t1 <- postForm(uri = ibmUrl, .params = body1, curl = handle)
handle <- getCurlHandle()
t1 <- postForm(uri = ibmUrl, .params = list(body1), curl = handle)
body1 <- "<Envelope><Body>
<Login>
<USERNAME>aliarsalan.kazmi@aimia.com</USERNAME>
<PASSWORD>Iamneo456_</PASSWORD>
</Login>
</Body></Envelope>"
test1 <- POST(url = ibmUrl,
body = body1, verbose(), content_type("application/x-www-form-urlencoded"))
test1
GET(ibmUrl)
r <- GET(ibmUrl)
status_code(r)
headers(r)
str(content(r))
http_status(r)
content(r)
content(r, encoding = "text")
content(r, encoding = "raw")
headers(r)
body1 <- "<Envelope><Body><Login><USERNAME>aliarsalan.kazmi@aimia.com</USERNAME><PASSWORD>Iamneo456_</PASSWORD></Login></Body></Envelope>"
t1 <- postForm(uri = ibmUrl, .params = body1, curl = handle)
test1 <- POST(url = ibmUrl,
body = body1, verbose(), content_type("application/x-www-form-urlencoded"))
content(test1)
test1 <- POST(url = ibmUrl,
body = body1, verbose(), content_type("application/x-www-form-urlencoded"))
test1 <- POST(url = ibmUrl,
body = body1, verbose(), content_type("text/xml"))
content(test1)
test1 <- POST(url = ibmUrl,
body = body1, verbose(), content_type("text/xml"))
content(test1)
?content
content(test1, as = "text")
content(test1, as = "raw")
content(test1, as = "parsed")
content(test1, type = xmlTreeParse)
content(test1, type = text/xml)
content(test1, type = "text/xml")
content(test1, type = "text/html")
content(test1, type = "text/csv")
content(test1, type = "text/csv")[6]
c1 <- content(test1, type = "text/csv")
c1[6]
c1[6,]
gsub(pattern = ".+=(.+)<", "\\1", c1[6,])
gsub(pattern = ".+=(.+)<.+", "\\1", c1[6,])
content(test1, type = "text/csv")[6,]
jsId <- gsub(pattern = ".+=(.+)<.+", "\\1", content(test1, type = "text/csv")[6,])
jsId
test1 <- POST(url = ibmUrl,
body = body1, verbose(), content_type("text/xml"))
jsId <- gsub(pattern = ".+=(.+)<.+", "\\1", content(test1, type = "text/csv")[6,])
body2 <- "<Envelope><Body>
<RawRecipientDataExport>
<EVENT_DATE_START>01/01/2015 00:00:00</EVENT_DATE_START>
<EVENT_DATE_END>01/01/2018 23:59:00</EVENT_DATE_END>
<MOVE_TO_FTP/>
<EXPORT_FORMAT>PIPE</EXPORT_FORMAT>
<EMAIL>aliarsalan.kazmi@aimia.com</EMAIL>
<ALL_EVENT_TYPES/>
<EXCLUDE_DELETED/>
<COLUMN>
<NAME>CustomerID</NAME>
</COLUMN>
<COLUMN>
<NAME>Address</NAME>
</COLUMN>
<RETURN_MAILING_NAME/>
</RawRecipientDataExport>
</Body></Envelope>"
test2 <- POST(url = ibmUrl, body = body2, verbose(), content_type("text/xml"), add_headers(jsessionid = jsId))
content(test2)
content(test1)$cookies
content(test1)
cache_info(test1)
rerequest(test1)
jsId <- gsub(pattern = ".+=(.+)<.+", "\\1", content(test1, type = "text/csv")[6,])
test1 <- POST(url = ibmUrl,
body = body1, verbose(), content_type("text/xml"))
jsId <- gsub(pattern = ".+=(.+)<.+", "\\1", content(test1, type = "text/csv")[6,])
httr_options
?httr_options
test1 <- POST(url = ibmUrl,
body = body1, verbose(), content_type("text/xml"), set_cookies())
jsId <- gsub(pattern = ".+=(.+)<.+", "\\1", content(test1, type = "text/csv")[6,])
cookies(test1)
r <- GET("http://httpbin.org/cookies/set", query = list(a = 1, b = 2))
cookies(r)
access_token
ibmUrl <- "https://api3.ibmmarketingcloud/oauth/token"
ibmUrl <- "https://api3.ibmmarketingcloud.com/oauth/token"
clientId <- "2fb47a91-3e34-4b2f-b9b5-93cffbf31500"
clientSecret <- "5b5aa1d0-9fcd-4740-b970-79b059c32a74"
refreshToken <- "rDI5XuVASIqwIOOeJdRHYC71vrLvci64kE1JgDipG8BAS1"
#Request authentication (access token)
reqAccess <- POST(url = ibmUrl,
body = list(grant_type = "refresh_token", client_id = clientId,
client_secret = clientSecret, refresh_token = refreshToken), verbose())
warn_for_status(reqAccess)
access_token <- content(reqAccess)$access_token
ibmUrl <- "api3.silverpop.com/XMLAPI"
body1 <- "<Envelope><Body>
<Login>
<USERNAME>aliarsalan.kazmi@aimia.com</USERNAME>
<PASSWORD>Iamneo456_</PASSWORD>
</Login>
</Body></Envelope>"
test1 <- POST(url = ibmUrl,
body = body1, verbose(), content_type("text/xml"))
jsId <- gsub(pattern = ".+=(.+)<.+", "\\1", content(test1, type = "text/csv")[6,])
body2 <- "<Envelope><Body>
<RawRecipientDataExport>
<EVENT_DATE_START>01/01/2015 00:00:00</EVENT_DATE_START>
<EVENT_DATE_END>01/01/2018 23:59:00</EVENT_DATE_END>
<MOVE_TO_FTP/>
<EXPORT_FORMAT>PIPE</EXPORT_FORMAT>
<EMAIL>aliarsalan.kazmi@aimia.com</EMAIL>
<ALL_EVENT_TYPES/>
<EXCLUDE_DELETED/>
<COLUMN>
<NAME>CustomerID</NAME>
</COLUMN>
<COLUMN>
<NAME>Address</NAME>
</COLUMN>
<RETURN_MAILING_NAME/>
</RawRecipientDataExport>
</Body></Envelope>"
test2 <- POST(url = ibmUrl, body = body2, verbose(), content_type("text/xml"), add_headers(jsessionid = jsId, Authorization = access_token))
content(test2)
paste0(ibmUrl, ";jsessionid=", jsId)
test1 <- POST(url = ibmUrl,
body = body1, verbose(), content_type("text/xml"))
jsId <- gsub(pattern = ".+=(.+)<.+", "\\1", content(test1, type = "text/csv")[6,])
body2 <- "<Envelope><Body>
<RawRecipientDataExport>
<EVENT_DATE_START>01/01/2015 00:00:00</EVENT_DATE_START>
<EVENT_DATE_END>01/01/2018 23:59:00</EVENT_DATE_END>
<MOVE_TO_FTP/>
<EXPORT_FORMAT>PIPE</EXPORT_FORMAT>
<EMAIL>aliarsalan.kazmi@aimia.com</EMAIL>
<ALL_EVENT_TYPES/>
<EXCLUDE_DELETED/>
<COLUMN>
<NAME>CustomerID</NAME>
</COLUMN>
<COLUMN>
<NAME>Address</NAME>
</COLUMN>
<RETURN_MAILING_NAME/>
</RawRecipientDataExport>
</Body></Envelope>"
test2 <- POST(url = paste0(ibmUrl, ";jsessionid=", jsId), body = body2, verbose(), content_type("text/xml"))
content(test2)
?source
match("ok", filesError)
?match
?vector
vector(mode = "character", length = 0)
library(repmis)
baseUrl <- "https://github.com/aliarsalankazmi/Aimia-Projects/Air-Miles/Air-Miles-Reboot/raw/master/Data"
source_data(paste0(baseUrl,'airmilesMEPageData.RData'))
baseUrl
packages <- c('Rfacebook', 'lubridate', 'plyr', 'dplyr', 'tidyr', 'xts', 'dygraphs', 'tm', 'twitteR', 'wordcloud', 'ggplot2', 'shinythemes', 'shiny', 'shinyapps', 'shinydashboard', 'stringr', 'topicmodels')
loaded <- sapply(packages, function(x) library(x, character.only = TRUE, quietly = TRUE, logical.return = TRUE))
if(all(loaded) != TRUE){
sapply(packages[!loaded], function(x) install.packages(x, dependencies = TRUE))
}
newLoaded <- sapply(packages[!loaded], function(x) library(x, character.only = TRUE, quietly = TRUE, logical.return = TRUE))
#-------------------- Setting the working directory ----------------------------------#
setwd('C:\\Users\\kazami\\Desktop\\Aimia\\AirMiles\\Task2-FacebookApp')
#-------------------- Setting Facebook Credentials --------------------------#
appID<- '998763900168964'
appSecret<- '7afa6fca1956a527b2456340cf8cc892'
#-------------------- Getting access token -------------------#
fb_token <- fbOAuth(app_id = appID, app_secret = appSecret, extended_permissions = TRUE)
load('fb_token')
#-------------------- Setting Twitter Credentials --------------------------#
apiKey <- "etjOWrV0DhWRdjnWpK4LDnZhF"
apiSecret <- "WaIS9j45da7UwgrB5DY19jCpez8ANZz0c1TBQ3H8iEoBXGVb0f"
accessToken <- "1471700995-kxZK5Kt8IhDs9Cetp78f8dM3l8pIW02cWZGMG7e"
tokenSecret <- "fZw8ggF1jkEi0hhiKUAFU0mxw7VwndSbSKxDXswFwcA"
#--------------------  Extracting Data from AirMiles ME Facebook Page -------------------------#
airmilesMe <- getPage(page = "AirMilesME", token = fb_token, n = 5000)
save(airmilesMe, file = 'airmilesMEPageData.RData')
postsIdOriginal <- airmilesMe$id
postsData <- lapply(postsIdOriginal, getPost, token = fb_token)
save(postsData, file = "postsData.RData")
##check all lists have 3 items
itemsCheck <- sapply(postsData, length)
summary(itemsCheck)
#nopes, some lists do not contain all elements (posts dataframe, likes dataframe, comments dataframe)
length(postsData)
itemsCheck1 <- unlist(sapply(postsData, names))
length(postsData) - sum(grepl("likes", x = itemsCheck1))
length(postsData) - sum(grepl("comments", x = itemsCheck1))
length(postsData) - sum(grepl("post", x = itemsCheck1))
#OK, so only likes dataframes are missing - 18 of these are missing.
itemsCheck2 <- which(!sapply(postsData, function(x) "likes" %in% names(x)))
#the above are lists not containing any liked items' list
emptyLikes <- postsData[itemsCheck2]
summary(sapply(emptyLikes, function(x) x$post$likes_count))
sapply(emptyLikes, function(x) x$post$likes_count)
#so most are zero, and rest are 1, but possibly left out due to user privacy
likedData <- postsData[-itemsCheck2]
postsIdOriginal1 <- postsIdOriginal[-itemsCheck2]
postsIds1 <- sapply(likedData, function(x) x$post$id)
length(postsIds1) == length(postsIdOriginal1)
which(postsIds1 != postsIdOriginal1)
#some post IDs are different from the original IDs, although they were extracted using the original post IDs! Leaving this as is for time being...
noOfLikes <- unlist(sapply(likedData, function(x) nrow(x$likes)))
created_time <- sapply(likedData, function(x) x$post$created_time)
postsIds <- rep(postsIds1, times = noOfLikes)
created_time <- rep(created_time, times = noOfLikes)
likedList <- lapply(likedData, function(x) x$likes)
likedUnlist <- do.call(rbind, likedList)
likedDataDf <- tbl_df(data.frame(postID = postsIds, created_time = created_time, stringsAsFactors = FALSE))
likedDataDf <- cbind(likedDataDf, likedUnlist)
likedDataDf <- tbl_df(likedDataDf)
likedDataDf$from_name <- factor(likedDataDf$from_name)
likedDataDf$created_date <- ymd(gsub("T.+", "", likedDataDf$created_time))
likedDataDf$year <- factor(year(likedDataDf$created_time))
likedDataDf$month <- month(likedDataDf$created_time)
likedDataDf$monthName <- factor(month(likedDataDf$created_time, label = TRUE))
likedDataDf$day <- factor(wday(likedDataDf$created_time))
likedDataDf$dayName <- factor(wday(likedDataDf$created_time, label = TRUE))
save(likedDataDf, file = "likedDataDf.RData")
noOfComments <- unlist(sapply(postsData, function(x) nrow(x$comments)))
requiredData <- postsData[noOfComments > 0]
newNoOfComments <- unlist(sapply(requiredData, function(x) nrow(x$comments)))
post_created_time <- as.Date(sapply(requiredData, function(x) x$post$created_time))
postsId <- sapply(requiredData, function(x) x$post$id)
post_created_time <- rep(post_created_time, times = newNoOfComments)
postsId <- rep(postsId, times = newNoOfComments)
commentsList <- lapply(requiredData, function(x) x$comments)
commentsUnlisted <- do.call(rbind, commentsList)
commentsDf <- data.frame(postId = postsId, post_time = post_created_time)
commentsDf <- cbind(commentsDf, commentsUnlisted)
commentsDf <- tbl_df(commentsDf)
commentsDf$created_time <- as.Date(commentsDf$created_time)
colnames(commentsDf) <- sub("created_time", "comment_time", colnames(commentsDf))
amComments <- commentsDf %>%
filter(from_name == "Air Miles Middle East")
amComments <- amComments$message
userComments <- commentsDf %>%
filter(from_name != "Air Miles Middle East") %>%
group_by(postId) %>%
summarise(totalComments = n(),
totalUniqueComments = n_distinct(from_name),
totalCommentLikes = sum(likes_count),
Comments = paste(message, collapse = " "),
durationTalkedAbout = max(comment_time) - min(post_time))
commentsVector <- userComments$Comments
commentsVector <- iconv(commentsVector, to = "ASCII", sub = "")
#userCommentsV <- gsub("[^A-Za-z0-9]", "", userCommentsV)
commentsVector <- sapply(commentsVector, function(x) gsub('[[:digit:]]', ' ', x))
commentsVector <- sapply(commentsVector, function(x) gsub('http(s)?[[:punct:]*]//.+?\\.(com|aspx|io|net|ly){0,1}\\.?(sa){0,1}.*?(.aspx){0,1}', ' ', x))
commentsVector <- sapply(commentsVector, function(x) gsub('shb.com.sa/m/\\b', '', x))
commentsVector <- sapply(commentsVector, function(x) gsub('\\n', ' ', x))
commentsVector <- sapply(commentsVector, function(x) gsub('[[:punct:]]', ' ', x))
commentsVector <- sapply(commentsVector, function(x) gsub('\\s+', ' ', x))
commentsVector <- sapply(commentsVector, tolower)
commentsVector <- sapply(commentsVector, function(x) gsub('\\s+', ' ', x))
commentsVector <- sapply(commentsVector, function(x) gsub('^[[:space:]]', '', x))
commentsVector <- sapply(commentsVector, function(x) gsub('$[[:space:]]', '', x))
## generating stopwords (English and Arabic)
engStopwords <- c(stopwords('english'), stopwords('SMART'), 'am', 'pm', 'air miles', 'aimia', 'spread', 'winner', 'air milers', 'dear', 'members', 'member', 'middle east', 'airmiles', 'and', 'winner', 'winners', 'today', 'middle', 'east')
## Replacing stopwords with whitespace
#x <- mapply(FUN = function(...){
#userCommentsClean <<- gsub(..., x = userCommentsV, replacement = " ")}, #the line before comma specifies the function for MAP, latter are arguments to be #repetitively used
#pattern = engStopwords)
#rm(x)
##Generating corpus
userCommentCorpus <- VCorpus(VectorSource(commentsVector))
userCommentCorpus <- tm_map(userCommentCorpus, removeWords, engStopwords)
##Generating Term Document Matrix
userCommentTdm <- TermDocumentMatrix(userCommentCorpus, control = list(weighting = function(x) weightTf(x), minWordLength = 3))
#Generating DFs for using with wordclouds
userCommentsDf <- data.frame(words = rownames(userCommentTdm), freq = rowSums(as.matrix(userCommentTdm)))
save(userCommentsDf, file = 'userComments.RData')
airmilesMe <- tbl_df(airmilesMe)
airmilesMe$created_date <- ymd(gsub('T.*', '', airmilesMe$created_time))
airmilesMe <- arrange(airmilesMe, created_date)
#airmilesMe$created_month <- factor(paste(month(airmilesMe$created_date),year(airmilesMe$created_date), sep = '-'))
airmilesMe$year_month <- factor(as.yearmon(airmilesMe$created_date))
airmilesMe$month <- month(airmilesMe$created_date)
airmilesMe$monthName <- factor(month(airmilesMe$created_date, label = TRUE))
airmilesMe$quarter <- factor(quarter(airmilesMe$created_date))
airmilesMe$year <- factor(year(airmilesMe$created_date))
airmilesMe$day <- factor(wday(airmilesMe$created_date))
airmilesMe$dayName <- factor(wday(airmilesMe$created_date, label = TRUE), ordered = TRUE)
airmilesMe$week <- week(airmilesMe$created_date)
#airmilesMe <- ddply(airmilesMe, .(year_month), transform, month_week = 1+ week - min(week)) from: http://margintale.blogspot.com/2012/04/ggplot2-time-series-heatmaps.html
pageData <- airmilesMe
likedMessages <- pageData[pageData$likes_count > mean(pageData$likes_count),]$message
sharedMessages <- pageData[pageData$shares_count > mean(pageData$shares_count),]$message
commentedMessages <- pageData[pageData$comments_count > mean(pageData$comments_count),]$message
corpus <- list(likedMessages, sharedMessages, commentedMessages)
corpus <- lapply(corpus, function(x) tolower(x))
corpus <- lapply(corpus, function(x) gsub('[[:digit:]]', ' ', x))
########corpus <- lapply(corpus, function(x) gsub('http(s)?[[:punct:]*]//.+?\\.(com|aspx|io|net){0,1}\\.?(sa){0,1}', '', x))
corpus <- lapply(corpus, function(x) gsub('http(s)?[[:punct:]*]//.+?\\.(com|aspx|io|net|ly){0,1}\\.?(sa){0,1}.*?(.aspx){0,1}', ' ', x))
corpus <- lapply(corpus, function(x) gsub('shb.com.sa/m/\\b', '', x))
corpus <- lapply(corpus, function(x) gsub('\\n', ' ', x))
corpus <- lapply(corpus, function(x) gsub('[[:punct:]]', ' ', x))
corpus <- lapply(corpus, function(x) gsub('\\s+', ' ', x))
corpus <- lapply(corpus, function(x) x[!is.na(x)])
corpus2 <- corpus
## generating stopwords (English and Arabic)
engStopwords <- c(stopwords('english'), stopwords('SMART'), 'am', 'pm', 'air miles', 'aimia', 'spread', 'winner', 'air milers', 'dear', 'members', 'member', 'www', 'airmilesme')
engStopwords <- gsub('^', '\\\\b', engStopwords)
engStopwords <- gsub('$', '\\\\b', engStopwords)
## Replacing stopwords with whitespace
x <- mapply(FUN = function(..., simplify = FALSE){
corpus2[[1]] <<- gsub(..., x = corpus2[[1]])}, #the line before comma specifies the function for MAP, latter are arguments to be repetitively used
pattern = engStopwords, replacement = ' ')
x <- mapply(FUN = function(..., simplify = FALSE){
corpus2[[2]] <<- gsub(..., x = corpus2[[2]])},
pattern = engStopwords, replacement = ' ')
x <- mapply(FUN = function(..., simplify = FALSE){
corpus2[[3]] <<- gsub(..., x = corpus2[[3]])},
pattern = engStopwords, replacement = ' ')
rm(x)
corpus2 <- lapply(corpus2, function(x) gsub('\\s+', ' ', x))
corpus2 <- lapply(corpus2, function(x) gsub('^[[:space:]]', '', x))
corpus2 <- lapply(corpus2, function(x) gsub('$[[:space:]]', '', x))
##Generating corpora
likedCorpus1 <- unique(corpus2[[1]])
sharedCorpus1 <- unique(corpus2[[2]])
commentedCorpus1 <- unique(corpus2[[3]])
likedCorpus <- VCorpus(VectorSource(likedCorpus1))
sharedCorpus <- VCorpus(VectorSource(sharedCorpus1))
commentedCorpus <- VCorpus(VectorSource(commentedCorpus1))
likedTdm <- TermDocumentMatrix(likedCorpus, control = list(weighting = function(x) weightTf(x), minWordLength = 3))
sharedTdm <- TermDocumentMatrix(sharedCorpus, control = list(weighting = function(x) weightTf(x), minWordLength = 3))
commentedTdm <- TermDocumentMatrix(commentedCorpus, control = list(weighting = function(x) weightTf(x), minWordLength = 3))
save(likedDf, file = 'wordDataLiked.RData')
save(sharedDf, file = 'wordDataShared.RData')
save(commentedDf, file = 'wordDataCommented.RData')
likedDf <- data.frame(words = rownames(likedTdm), freq = rowSums(as.matrix(likedTdm)))
sharedDf <- data.frame(words = rownames(sharedTdm), freq = rowSums(as.matrix(sharedTdm)))
commentedDf <- data.frame(words = rownames(commentedTdm), freq = rowSums(as.matrix(commentedTdm)))
save(likedDf, file = 'wordDataLiked.RData')
save(sharedDf, file = 'wordDataShared.RData')
save(commentedDf, file = 'wordDataCommented.RData')
setup_twitter_oauth(apiKey, apiSecret, accessToken, tokenSecret)
am <- getUser("airmiles_me")
amTw <- userTimeline("airmiles_me", n = 5000)
amTwDf <- twListToDF(amTw)
amTwDf$year <- year(amTwDf$created)
amTwDf$quarter <- quarter(amTwDf$created)
amTwDf$month <- month(amTwDf$created)
amTwDf$monthName <- month(amTwDf$created, label = TRUE)
amTwDf$dayName <- wday(amTwDf$created, label = TRUE)
twitterData <- amTwDf
save(twitterData, file = 'twitterPage.RData')
mostFavPosts <- twitterData %>%
filter(favoriteCount >= mean(favoriteCount)) %>%
select(text) %>%
DataframeSource() %>%
Corpus() %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords,  c(stopwords("english"), stopwords("SMART"), "air", "miles")) %>%
tm_map(stripWhitespace) %>%
TermDocumentMatrix(control = list(weighting = function(x) weightTf(x), minWordLength = 3))
mostFavDf <- data.frame(words = rownames(mostFavPosts), freq = rowSums(as.matrix(mostFavPosts)))
mostRetweetPosts <- twitterData %>%
filter(favoriteCount >= mean(retweetCount)) %>%
select(text) %>%
DataframeSource() %>%
Corpus() %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords,  c(stopwords("english"), stopwords("SMART"), "air", "miles")) %>%
tm_map(stripWhitespace) %>%
TermDocumentMatrix(control = list(weighting = function(x) weightTf(x), minWordLength = 3))
mostRetweetDf <- data.frame(words = rownames(mostRetweetPosts), freq = rowSums(as.matrix(mostRetweetPosts)))
save(mostFavDf, file = 'mostFavDf.RData')
save(mostRetweetDf, file = 'mostRetweetDf.RData')
rm(list = ls())
baseUrl <- "https://github.com/aliarsalankazmi/Aimia-Projects/Air-Miles/Air-Miles-Reboot/raw/master/Data"
source_data(paste0(baseUrl,'airmilesMEPageData.RData'))
baseUrl <- "https://github.com/aliarsalankazmi/Aimia-Projects/Air-Miles/Air-Miles-Reboot/raw/master/Data"
source_data(paste0(baseUrl,'airmilesMEPageData.RData'))
baseUrl <- "https://github.com/aliarsalankazmi/Aimia-Projects/Air-Miles/Air-Miles-Reboot/Data/"
source_data(paste0(baseUrl,'airmilesMEPageData.RData'))
source_data("https://github.com/aliarsalankazmi/Aimia-Projects/raw/master/Air-Miles/Air-Miles-Reboot/Data/airmilesMEPageData.RData")
baseUrl <- "https://github.com/aliarsalankazmi/Aimia-Projects/raw/master/Air-Miles/Air-Miles-Reboot/Data/"
source_data(paste0(baseUrl,'airmilesMEPageData.RData'))
source_data(paste0(baseUrl,'wordDataCommented.RData'))
source_data(paste0(baseUrl,'wordDataLiked.RData'))
source_data(paste0(baseUrl,'wordDataShared.RData'))
source_data(paste0(baseUrl, 'userComments.RData'))
source_data(paste0(baseUrl, 'likedDataDf.RData'))
source_data(paste0(baseUrl, 'twitterPage.RData'))
source_data(paste0(baseUrl, 'mostFavDf.RData'))
source_data(paste0(baseUrl, 'mostRetweetDf.RData'))
rm(list = ls())
setwd("C:\\Users\\kazami\\Documents\\GitHub\\Aimia-Projects\\Air-Miles\\Air-Miles-Reboot")
runApp()
baseUrl <- "https://github.com/aliarsalankazmi/Aimia-Projects/raw/master/Air-Miles/Air-Miles-Reboot/Data/"
source_data(paste0(baseUrl,'airmilesMEPageData.RData'))
source_data(paste0(baseUrl,'wordDataCommented.RData'))
rm(list = ls())
runApp()
getwd()
